from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import json
import time
import re
import os
import argparse
from datetime import datetime
import sys
from tqdm import tqdm
from parser import APIParser, DataExporter

def get_api_id(url):
    """URLì—ì„œ API ID ì¶”ì¶œ"""
    m = re.search(r'/data/(\d+)/openapi', url)
    return m.group(1) if m else f"api_{url.replace('https://', '').replace('/', '_')}"

def read_urls(file_path):
    """URL ëª©ë¡ íŒŒì¼ ì½ê¸°"""
    with open(file_path, 'r', encoding='utf-8') as f:
        return [line.strip() for line in f if line.strip() and not line.startswith('#')]

def setup_driver():
    """Selenium WebDriver ì„¤ì •"""
    opts = Options()
    opts.add_argument("--headless")
    opts.add_argument("--no-sandbox")
    opts.add_argument("--disable-dev-shm-usage")
    opts.add_argument("--disable-gpu")
    opts.add_argument("--disable-software-rasterizer")
    opts.add_argument("--disable-features=VizDisplayCompositor")
    opts.add_argument("--disable-gl-drawing-for-tests")
    opts.add_argument("--disable-webgl")
    opts.add_argument("--disable-webgl2")
    opts.add_argument("--use-gl=swiftshader")
    opts.add_argument("--disable-usb-keyboard-detect")
    opts.add_argument("--disable-features=WebUSB")
    opts.add_argument("--log-level=3")
    opts.add_experimental_option('excludeSwitches', ['enable-logging'])
    opts.add_experimental_option('useAutomationExtension', False)
    opts.add_experimental_option('prefs', {
        'profile.default_content_setting_values.usb_guard': 2
    })
    return webdriver.Chrome(options=opts)

def crawl_api(url, output_dir="api_docs", formats=['json', 'xml', 'md'], custom_filename=None):
    """API í¬ë¡¤ë§ ë° ì €ì¥ - Base URL ì¶”ì¶œ ê¸°ëŠ¥ ê°œì„ """
    os.makedirs(output_dir, exist_ok=True)
    api_id = get_api_id(url) if not custom_filename else custom_filename
    
    driver = setup_driver()
    crawling_result = {
        'success': False,
        'data': None,
        'saved_files': [],
        'errors': [],
        'api_id': api_id,
        'url': url
    }
    
    try:
        print(f"ğŸ” í¬ë¡¤ë§ ì‹œì‘: {url}")
        driver.get(url)
        
        try:
            # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸° - ì—¬ëŸ¬ ë°©ë²•ìœ¼ë¡œ ì‹œë„
            wait_methods = [
                (By.CLASS_NAME, "swagger-ui"),
                (By.CLASS_NAME, "open-api-title"),
                (By.TAG_NAME, "body")
            ]
            
            page_loaded = False
            for method in wait_methods:
                try:
                    WebDriverWait(driver, 20).until(EC.presence_of_element_located(method))
                    page_loaded = True
                    print("âœ“ í˜ì´ì§€ ë¡œë”© ì™„ë£Œ")
                    break
                except:
                    continue
            
            if not page_loaded:
                print("âš ï¸  í˜ì´ì§€ ë¡œë”© í™•ì¸ ì‹¤íŒ¨, ê³„ì† ì§„í–‰...")
            
            time.sleep(5)  # ì¶”ê°€ ëŒ€ê¸° ì‹œê°„
        except Exception as e:
            print(f"âš ï¸  í˜ì´ì§€ ë¡œë”© ëŒ€ê¸° ì¤‘ ì˜¤ë¥˜: {e}")
            time.sleep(3)
        
        # íŒŒì„œ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        parser = APIParser(driver)
        
        # API ì •ë³´ ì¶”ì¶œ
        print("ğŸ“Š API ì •ë³´ ì¶”ì¶œ ì¤‘...")
        api_info = parser.extract_api_info()
        api_info = parser.extract_meta_info(api_info)
        
        # Base URL ì¶”ì¶œ (ê°œì„ ëœ ë¡œì§)
        print("ğŸ”— Base URL ì¶”ì¶œ ì¤‘...")
        base_url = parser.extract_base_url()
        api_info['base_url'] = base_url
        
        # Schemes ì¶”ì¶œ
        api_info['schemes'] = parser.extract_schemes()
        
        # ì—”ë“œí¬ì¸íŠ¸ ì¶”ì¶œ
        print("ğŸ”— ì—”ë“œí¬ì¸íŠ¸ ì¶”ì¶œ ì¤‘...")
        endpoints = parser.extract_endpoints()
        
        # ê²°ê³¼ ë°ì´í„° êµ¬ì„±
        result = {
            'api_info': api_info,
            'endpoints': endpoints,
            'crawled_url': url,
            'crawled_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'api_id': api_id
        }
        
        crawling_result['data'] = result
        crawling_result['success'] = True
        
        # ë°ì´í„° ì €ì¥ (JSON, XML)
        print("ğŸ’¾ ë°ì´í„° ì €ì¥ ì¤‘...")
        saved_files, save_errors = DataExporter.save_crawling_result(result, output_dir, api_id, formats)
        
        crawling_result['saved_files'] = saved_files
        crawling_result['errors'] = save_errors
        
        # ì €ì¥ëœ íŒŒì¼ ê²€ì¦
        if saved_files:
            print("ğŸ” íŒŒì¼ ìœ íš¨ì„± ê²€ì¦ ì¤‘...")
            validation_results = DataExporter.validate_saved_files(saved_files)
            
            valid_files = [f for f, r in validation_results.items() if r.get('valid', False)]
            invalid_files = [f for f, r in validation_results.items() if not r.get('valid', False)]
            
            if valid_files:
                print(f"âœ… ê²€ì¦ ì™„ë£Œ: {len(valid_files)}ê°œ íŒŒì¼ì´ ìœ íš¨í•¨")
                for file_name, result_info in validation_results.items():
                    if result_info.get('valid', False):
                        size_kb = result_info.get('size', 0) / 1024
                        print(f"   ğŸ“„ {file_name} ({size_kb:.1f}KB)")
            
            if invalid_files:
                print(f"âŒ ê²€ì¦ ì‹¤íŒ¨: {len(invalid_files)}ê°œ íŒŒì¼ì— ë¬¸ì œ ë°œê²¬")
                for file_name, result_info in validation_results.items():
                    if not result_info.get('valid', False):
                        print(f"   âŒ {file_name}: {result_info.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}")
        
        # ê²°ê³¼ ìš”ì•½
        if saved_files and not save_errors:
            print(f"ğŸ‰ í¬ë¡¤ë§ ì™„ë£Œ!")
            print(f"   ğŸ“‹ API: {result['api_info'].get('title', 'N/A')}")
            print(f"   ğŸŒ Base URL: {result['api_info'].get('base_url', 'N/A')}")
            print(f"   ğŸ”— ì—”ë“œí¬ì¸íŠ¸: {len(result['endpoints'])}ê°œ")
            print(f"   ğŸ“ ì €ì¥ íŒŒì¼: {len(saved_files)}ê°œ")
        elif saved_files and save_errors:
            print(f"âš ï¸  ë¶€ë¶„ ì„±ê³µ:")
            print(f"   ğŸ“‹ API: {result['api_info'].get('title', 'N/A')}")
            print(f"   ğŸŒ Base URL: {result['api_info'].get('base_url', 'N/A')}")
            print(f"   ğŸ”— ì—”ë“œí¬ì¸íŠ¸: {len(result['endpoints'])}ê°œ")
            print(f"   âœ… ì„±ê³µ: {len(saved_files)}ê°œ íŒŒì¼")
            print(f"   âŒ ì‹¤íŒ¨: {len(save_errors)}ê°œ ì˜¤ë¥˜")
            for error in save_errors:
                print(f"      - {error}")
        else:
            print(f"âŒ ì €ì¥ ì‹¤íŒ¨:")
            for error in save_errors:
                print(f"   - {error}")
            crawling_result['success'] = False
        
        return crawling_result
    
    except Exception as e:
        error_msg = f"í¬ë¡¤ë§ ì‹¤íŒ¨: {str(e)}"
        print(f"âŒ {error_msg}")
        crawling_result['errors'].append(error_msg)
        return crawling_result
    finally:
        driver.quit()

def batch_crawl(file_path, output_dir="api_docs", formats=['json', 'xml', 'md']):
    """ë°°ì¹˜ í¬ë¡¤ë§ - ê°œì„ ëœ ë²„ì „"""
    urls = read_urls(file_path)
    os.makedirs(output_dir, exist_ok=True)
    
    summary = {
        "total": len(urls),
        "success": 0,
        "partial_success": 0,
        "failed": 0,
        "failed_urls": [],
        "success_details": [],
        "start_time": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        "output_formats": formats,
        "output_directory": output_dir
    }
    
    print(f"ğŸš€ ë°°ì¹˜ í¬ë¡¤ë§ ì‹œì‘")
    print(f"   ğŸ“‹ ì´ {len(urls)}ê°œ URL")
    print(f"   ğŸ“ ì¶œë ¥ ë””ë ‰í† ë¦¬: {output_dir}")
    print(f"   ğŸ’¾ ì €ì¥ í˜•ì‹: {', '.join(formats)}")
    print("=" * 50)
    
    for i, url in enumerate(tqdm(urls, desc="í¬ë¡¤ë§ ì§„í–‰"), 1):
        print(f"\n[{i}/{len(urls)}] ì²˜ë¦¬ ì¤‘...")
        
        try:
            result = crawl_api(url, output_dir, formats)
            
            if result['success'] and result['saved_files'] and not result['errors']:
                # ì™„ì „ ì„±ê³µ
                summary["success"] += 1
                success_detail = {
                    "url": url,
                    "api_id": result['api_id'],
                    "title": result['data']['api_info'].get('title', 'N/A'),
                    "base_url": result['data']['api_info'].get('base_url', 'N/A'),
                    "endpoints_count": len(result['data'].get('endpoints', [])),
                    "saved_files": [os.path.basename(f) for f in result['saved_files']],
                    "status": "ì™„ì „ ì„±ê³µ"
                }
                summary["success_details"].append(success_detail)
                
            elif result['success'] and result['saved_files'] and result['errors']:
                # ë¶€ë¶„ ì„±ê³µ
                summary["partial_success"] += 1
                success_detail = {
                    "url": url,
                    "api_id": result['api_id'],
                    "title": result['data']['api_info'].get('title', 'N/A'),
                    "base_url": result['data']['api_info'].get('base_url', 'N/A'),
                    "endpoints_count": len(result['data'].get('endpoints', [])),
                    "saved_files": [os.path.basename(f) for f in result['saved_files']],
                    "save_errors": result['errors'],
                    "status": "ë¶€ë¶„ ì„±ê³µ"
                }
                summary["success_details"].append(success_detail)
                
            else:
                # ì‹¤íŒ¨
                summary["failed"] += 1
                summary["failed_urls"].append({
                    "url": url,
                    "api_id": result.get('api_id', 'unknown'),
                    "reason": "; ".join(result['errors']) if result['errors'] else "ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜"
                })
                
        except Exception as e:
            summary["failed"] += 1
            summary["failed_urls"].append({
                "url": url,
                "api_id": get_api_id(url),
                "reason": f"ì˜ˆì™¸ ë°œìƒ: {str(e)}"
            })
        
        # URL ê°„ ëŒ€ê¸° ì‹œê°„
        if i < len(urls):
            time.sleep(2)
    
    # ë°°ì¹˜ í¬ë¡¤ë§ ì™„ë£Œ ì²˜ë¦¬
    summary["end_time"] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    
    # ì„±ê³µë¥  ê³„ì‚°
    total_attempted = summary['total']
    if total_attempted > 0:
        success_rate = (summary['success'] + summary['partial_success']) / total_attempted * 100
        summary["success_rate"] = f"{success_rate:.1f}%"
        summary["complete_success_rate"] = f"{(summary['success'] / total_attempted * 100):.1f}%"
    else:
        summary["success_rate"] = "0%"
        summary["complete_success_rate"] = "0%"
    
    # ìš”ì•½ ì •ë³´ ì €ì¥
    summary_file = os.path.join(output_dir, "crawling_summary.json")
    success, error = DataExporter.save_as_json(summary, summary_file)
    if not success:
        print(f"âš ï¸  ìš”ì•½ íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {error}")
    
    # ì‹¤íŒ¨í•œ URL ëª©ë¡ ì €ì¥ (ì¬ì‹œë„ìš©)
    if summary["failed_urls"]:
        failed_urls_file = os.path.join(output_dir, "failed_urls.txt")
        try:
            with open(failed_urls_file, 'w', encoding='utf-8') as f:
                for item in summary["failed_urls"]:
                    f.write(f"{item['url']}\n")
            print(f"ğŸ“„ ì‹¤íŒ¨ URL ëª©ë¡ ì €ì¥: {failed_urls_file}")
        except Exception as e:
            print(f"âš ï¸  ì‹¤íŒ¨ URL ëª©ë¡ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    # ìµœì¢… ê²°ê³¼ ì¶œë ¥
    print("\n" + "=" * 50)
    print("ğŸ ë°°ì¹˜ í¬ë¡¤ë§ ì™„ë£Œ!")
    print("=" * 50)
    print(f"ğŸ“Š ì „ì²´ ê²°ê³¼:")
    print(f"   ğŸ“‹ ì´ ì²˜ë¦¬: {summary['total']}ê°œ URL")
    print(f"   âœ… ì™„ì „ ì„±ê³µ: {summary['success']}ê°œ ({summary['complete_success_rate']})")
    print(f"   âš ï¸  ë¶€ë¶„ ì„±ê³µ: {summary['partial_success']}ê°œ")
    print(f"   âŒ ì‹¤íŒ¨: {summary['failed']}ê°œ")
    print(f"   ğŸ“ˆ ì „ì²´ ì„±ê³µë¥ : {summary['success_rate']}")
    print(f"   ğŸ’¾ ì €ì¥ í˜•ì‹: {', '.join(formats)}")
    print(f"   ğŸ“ ê²°ê³¼ ìœ„ì¹˜: {output_dir}")
    
    if success:
        print(f"   ğŸ“‹ ìš”ì•½ íŒŒì¼: crawling_summary.json")
    if summary["failed"] > 0:
        print(f"   ğŸ“„ ì‹¤íŒ¨ ëª©ë¡: failed_urls.txt")

def main():
    parser = argparse.ArgumentParser(description='ê³µê³µë°ì´í„°í¬í„¸ API í¬ë¡¤ëŸ¬ (Base URL ì¶”ì¶œ ê¸°ëŠ¥ ê°œì„ )')
    parser.add_argument('url', nargs='?', help='API URL')
    parser.add_argument('-f', '--file', help='URL ëª©ë¡ íŒŒì¼')
    parser.add_argument('-o', '--output', help='ì¶œë ¥ íŒŒì¼ (ë‹¨ì¼ í¬ë¡¤ë§ ì‹œ)')
    parser.add_argument('-d', '--dir', default='api_docs', help='ì¶œë ¥ ë””ë ‰í† ë¦¬')
    parser.add_argument('--format', 
                       choices=['json', 'xml', 'md', 'markdown', 'all'], 
                       default='all',
                       help='ì¶œë ¥ í˜•ì‹ ì„ íƒ (ê¸°ë³¸ê°’: all)')
    
    args = parser.parse_args()
    
    if not args.url and not args.file:
        parser.print_help()
        sys.exit(1)
    
    # ì¶œë ¥ í˜•ì‹ ì„¤ì •
    if args.format == 'all':
        formats = ['json', 'xml', 'md']
    elif args.format in ['md', 'markdown']:
        formats = ['json', 'md']  # Markdownì€ JSONì„ ë¨¼ì € ìƒì„±í•œ í›„ ë³€í™˜
    else:
        formats = [args.format]
    
    if args.file:
        batch_crawl(args.file, args.dir, formats)
    else:
        # ë‹¨ì¼ í¬ë¡¤ë§ ì‹œ ì‚¬ìš©ì ì§€ì • íŒŒì¼ëª… ì²˜ë¦¬
        custom_filename = None
        if args.output:
            # í™•ì¥ì ì œê±°
            custom_filename = os.path.splitext(os.path.basename(args.output))[0]
        
        result = crawl_api(args.url, args.dir, formats, custom_filename)
        if not result['success'] or not result['saved_files']:
            print("âŒ í¬ë¡¤ë§ ì‹¤íŒ¨. ìœ„ì˜ ì˜¤ë¥˜ ë©”ì‹œì§€ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
            sys.exit(1)
        else:
            print("âœ… ë‹¨ì¼ í¬ë¡¤ë§ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œ!")

if __name__ == "__main__":
    main()